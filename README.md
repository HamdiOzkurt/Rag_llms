RAG LLM DokÃ¼man Analiz AracÄ±
Bu proje, yerel LLM (Large Language Model) modellerini kullanarak yÃ¼klenen PDF dokÃ¼manlarÄ± Ã¼zerinde soru-cevap yapmanÄ±zÄ± ve bilgi Ã§Ä±karmanÄ±zÄ± saÄŸlayan bir Streamlit uygulamasÄ±dÄ±r.

Uygulama, RAG (Retrieval-Augmented Generation) mimarisini kullanÄ±r. YÃ¼klenen PDF dokÃ¼manlarÄ±nÄ± parÃ§alar, vektÃ¶r veritabanÄ±na (ChromaDB) kaydeder ve Ollama Ã¼zerinden Ã§alÄ±ÅŸan yerel bir LLM modeli ile sorularÄ± yanÄ±tlar.

ğŸš€ Ã–zellikler
PDF YÃ¼kleme ve Ã–nizleme: KullanÄ±cÄ± dostu arayÃ¼z ile PDF yÃ¼kleme ve gÃ¶rÃ¼ntÃ¼leme.
VektÃ¶r VeritabanÄ±: DokÃ¼manlar parÃ§alanarak ChromaDB Ã¼zerinde vektÃ¶rleÅŸtirilir ve saklanÄ±r.
Yerel LLM DesteÄŸi: Ollama entegrasyonu sayesinde verileriniz dÄ±ÅŸarÄ± Ã§Ä±kmadan yerel modelinizle Ã§alÄ±ÅŸÄ±r.
Otomatik Bilgi Ã‡Ä±karÄ±mÄ±: AraÅŸtÄ±rma kaÄŸÄ±tlarÄ±ndan baÅŸlÄ±k, Ã¶zet, yayÄ±n tarihi ve yazar bilgilerini otomatik olarak Ã§Ä±karÄ±r.
ğŸ› ï¸ Teknolojiler
ArayÃ¼z: Streamlit
Orkestrasyon: LangChain
VektÃ¶r VeritabanÄ±: ChromaDB
LLM Sunucusu: Ollama
Konteynerizasyon: Docker & Docker Compose
ğŸ“‹ Gereksinimler
Docker ve Docker Compose (Ã–nerilen)
VEYA Python 3.9+ ve yerel olarak Ã§alÄ±ÅŸan bir Ollama kurulumu
ğŸ³ Docker ile Kurulum (Ã–nerilen)
En kolay kurulum yÃ¶ntemidir. TÃ¼m servisleri (Uygulama ve Ollama) tek komutla ayaÄŸa kaldÄ±rÄ±r.

Repoyu klonlayÄ±n veya indirin:

git clone <repo-url>
cd rag_llms
UygulamayÄ± baÅŸlatÄ±n:

docker-compose up --build
Not: docker-compose.yml dosyasÄ±ndaki Ollama volume ayarÄ± (C:\Users\hamdi\.ollama:/root/.ollama) Windows yoluna gÃ¶re ayarlanmÄ±ÅŸtÄ±r. Kendi bilgisayarÄ±nÄ±zdaki Ollama modellerinin bulunduÄŸu yolu buraya verebilir veya bu satÄ±rÄ± kaldÄ±rarak container iÃ§inde modelleri tekrar indirebilirsiniz.

Uygulamaya eriÅŸin: TarayÄ±cÄ±nÄ±zda http://localhost:8501 adresine gidin.

ğŸ Yerel Kurulum (Python)
Docker kullanmadan Ã§alÄ±ÅŸtÄ±rmak isterseniz:

Sanal ortam oluÅŸturun ve aktif edin:

python -m venv venv
# Windows iÃ§in:
.\venv\Scripts\activate
# Linux/Mac iÃ§in:
source venv/bin/activate
BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:

pip install -r requirements.txt
Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: BilgisayarÄ±nÄ±zda Ollama'yÄ± kurun ve Ã§alÄ±ÅŸtÄ±rÄ±n.

ollama serve
Gerekli modeli indirdiÄŸinizden emin olun (kod iÃ§inde varsayÄ±lan olarak kullanÄ±lan model, Ã¶rn: mistral, llama veya nomic-embed-text vb.).

UygulamayÄ± baÅŸlatÄ±n:

streamlit run app/streamlit_app.py
ğŸ“‚ Proje YapÄ±sÄ±
rag_llms/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ db/               # ChromaDB veritabanÄ± dosyalarÄ±
â”‚   â”œâ”€â”€ functions.py      # RAG pipeline, Embedding ve LLM fonksiyonlarÄ±
â”‚   â””â”€â”€ streamlit_app.py  # Ana uygulama arayÃ¼zÃ¼
â”œâ”€â”€ data/                 # Ã–rnek veri klasÃ¶rÃ¼
â”œâ”€â”€ docker-compose.yml    # Docker servis tanÄ±mlarÄ±
â”œâ”€â”€ dockerfile            # Uygulama iÃ§in Docker imaj dosyasÄ±
â”œâ”€â”€ requirements.txt      # Python kÃ¼tÃ¼phaneleri
â””â”€â”€ README.md             # DokÃ¼mantasyon
âš™ï¸ KonfigÃ¼rasyon
Uygulama app/functions.py iÃ§erisindeki get_ollama_base_url fonksiyonu ile Ã§alÄ±ÅŸma ortamÄ±nÄ± (Docker veya Local) otomatik algÄ±lar.

Docker OrtamÄ±: http://ollama:11434 adresine baÄŸlanÄ±r.
Local Ortam: http://localhost:11434 adresine baÄŸlanÄ±r.
EÄŸer farklÄ± bir URL kullanmak isterseniz OLLAMA_BASE_URL Ã§evre deÄŸiÅŸkenini (environment variable) ayarlayabilirsiniz.
